---
title: "Deep learning Basic- tensorflow"
description: "Note summary from Deep Learning with Python from Francois Chollet"
author: "Sunny Hospital"
date: "2024-04-10"
categories: ["data science",  "python"]
image: "cat.jpeg"
draft: true
---

## Tensorflow/Keras
* Tensorflow is deep learning platform that allows users to manipulate data (tensors) similar to numpy.
* Keras is an API for humans, not machine, and it allows you to build and train network


## tensorflow

To create a tensor, you start with ones, zeros, random values.
But they are Constant and cannot be assigned. 

To store value updates during training, ou need to use tf.Variable which is
assignable.

### Tensor basic
``` 
import tensorflow

x = tf.ones(shape = (2,1))
x = tf.zeros(shape = (2,1))

# random values
x = tf.random.normal(shape=(2,1), mean=0., stddev = 1.)
x = tf.random.uniform(shape=(3,1), minval=0., maxval = 1.)

# unlike numpy, tensors are not assignable.  they are Constant. 
# fails to do   x(0, 0] = 0.

# random variable
v= tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
v.assign(tf.ones((3, 1)))

# assign subset
v[0, 0].assign(3.)

# += and -=
v.assign_add(tf.ones((3,1)))
v.assign_sub(tf.ones((3,1)))

```
### Tensor Operations (math in tensorflow)

_eager execution_ is when operations arer executed on the fly

``` 
a = tf.ones((2,2))
b = tf.square(a)
c = tf.sqrt(a)
d = b + c # (element-wise addition)
e = tf.matmul(a,b) # product of two matrices
e *= d      # element-wise multiplication
```
### Differentials 

Tensorflow can retrieve the gradient of any differentiable experessions with respect to 
inputs. It can even compute second order gradients

``` 

input_var = tf.Variable(initial_value = 3.)
with tf.GradientTape() as tape:
    result = tf.square(input_var)
gradient = tape.gradient(result, input_var)

# tape.gradient(loss, weight) 

```

* Inputs can be any arbitrary tensor
* For efficiently only trainable variables are tracked by default and not constant

```
input_var = tf.constant(3.)
with tf.GradientTape() as tape:
    tape.watch(input_var)   # manually tracked
    result = tf.square(input_var)

gradient = tape.gradient(result, input_const)
```

### Example of linear classifier

__Set up variables__

```
num_samples_per_class = 1000
negatvie_samples = np.random.multivariate_normal(   # (1000,2)
    mean = [0, 3],
    cov = [[1, 0.5], [0.5, 1]],             
    size = num_samples_per_class)
positive_samples = np.random.multivariate_normal(   # (1000,2)
    mean = [3, 0],
    cov = [[1, 0.5], [0.5, 1]],
    size = num_samples_per_class)
)

# stack two samples to (2000, 2)
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)

# create corresponding target labels (2000, 1) 
# zeros and ones (2 classes) belong to 2 diff distributions
targets = np.vstack((np.zeros((num_samples_per_class , 1), dtype="float32"),
                    np.ones((num_samples_per_class, 1), dtype = "float32"))
                    )

# plot
import matplotlib.pyplot as plt

plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:,0])
plt.show()

# create linear classifier 
# y = W*x + basic
input_dim = 2   # x
output_dim = 1  # y

W = tf.Variable(initial_value = tf.random.uniform(shape = (input_dim, output_dim)))
b = tf.Variable(initial_value = tf.zeros(shape = (output_dim, )))

```
 
__ Set up model __

```
def model(inputs):
    return tf.matmul(inputs, W) + b

def square_loss(targets, predictions):

    # loss function squared of (x-x_bar)
    per_sample_losses = tf.square(targets - predictions)

learning_rate = 0.1

def training_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs) 
        loss = square_loss(targets, predictions)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate) # update W
    b.assign_sub(grad_loss_wrt_b * learning_rate) # update b

```

__Batch training__

```
for step in range(40):
    loss = training_step(inputs, targets)
    print(f"loss at step (step): {loss: .4f}")
```



:::{.callout-warning title="TODO"}
 
:::