[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is a space where I add notes about something I learn."
  },
  {
    "objectID": "posts/blog-post/index.html",
    "href": "posts/blog-post/index.html",
    "title": "Lit Review: Comparing resampling methods",
    "section": "",
    "text": "Article: Validation of machine learning ridge regression models using Monte Carlo, bootstrap, and variations in cross-validation Link to the article\nThe article compares performance of the commonly used resampling techniques in machine/statistical learning using ridge regression models. There are many conflicting suggestions for methods and parameter values for fine tuning hyper parameters.\nUsing simulations, the author tries to answer the following questions:\n\nWhich of the four resampling methods is most effective in selecting a suitable regulatiziation paremter \\(\\lambda\\)?\nDoes increasing the number of repetitions - from 10 to 50 - improve the performance of the resampling method?\nKeeping the number of repetitions constant, which approach, single-run cross-validation or repated cross-validation, performs better?\nFor k=fold CV, what is an appropriate fold size k?\nWhich randomization approach is more effective, Monte Carlo (sampling without replacement) or bootstrap (with replacement)?\n\n\nRidge regression model\nIn regression model where Y is the target (response) variable and X_1, X_2, ..XP are feature (independent) variables, our goal is to find _beta coefficients that minimize the SSE in OLS. Linear model could have an overfitting problem.\nTo deal with overfitting issue in linear models, the ridge regression model was used to apply regularization. The penality term was added to the SSE equation.\n\\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} ({\\beta}_j^2)\n\\]\nThe ridge regression model and MSE are used to find the lambda estimate. Data set is split into a training and validation sets where training set is used for parameter estimation and the validation set for evaluating overfit. One split of a dataset could create bias in MSE thus resampling methods are used to obtain MSE estimates.\nRegularization results in shiriking the \\(\\beta_j\\) toward 0 as \\(\\lambda\\) becomes large (or \\(\\beta\\) is same when \\(\\lambda\\) =0).\n\n\nResampling methods\n\nMonte Carlo\n(Other names: repeated learning-testing, repeated holdout, random subsampling)\nMonte Carlo sampling randomly shuffles the dataset (randomizing rows) and select the first 75% to be a training set and the remaining 25% as a validation set. This is sampling without replacement.\nThe validation error is averaged over n repetitions.\n\n\nBootstrap\nSimilar to Monte Carlo sampling where data are randomly selected from the dataset but with replacement.\nThe validation error is averaged over n repetitions.\n\n\n\n\n\n\nSunnys note\n\n\n\ncommonly (or by default) equal sampling probability?\n\n\n\n\nk-fold CV (Cross-validation)\nA dataset is divided into k partition at random (most commonly k=5 or 10). One partition is held out (set aside) as a validation set, and the remaining k-1 partitions of data are used as a training set. __The validation error_ is averaged over k repetitions.\nMost extreme case of k-fold CV, known as LOOCV (Leave One Out CV) is k = n where 1 observation is held out at each iteration.\n\n\nrepeated k-fold CV\nk-fold CV is done over n repetition and obtain \\({n} x {k}\\) validation errors.\nThe validation error is averaged over \\({n} \\times {k}\\)\n\n\n\n\n\n\nNote\n\n\n\nmost commonly use k=fold CV only once (rep =1) and repeated method is suggested as a way of obtaining more accurate and reliable estimates of error rates"
  },
  {
    "objectID": "posts/blog-post/pycascade24.html",
    "href": "posts/blog-post/pycascade24.html",
    "title": "PyCascades 2024",
    "section": "",
    "text": "Typing concepts/categories\n\nstrongly vs weakly (“loosely”)\ndynamically vs statically\nDuck typing and use of dunder\n\nPython best practices\n\nPython likes protocols/strongly type (type hinting/duck typing)\nAnnotate it as concrete as possible\nDeclarative data Types\n\n    def func() -&gt; list:  \n    def func() -&gt; list[str]:\n\nPermissive input types and strict return types\nUse standard libraries\nIf you are creating for publishing for other people, more typed the better\nBut.. if it doesn’t work for you, don’t do it.\n\n\n\n\n\nFrom bugs, results and analysis of the bugs, we can learn so much and improve coding practices. The speaker presented 3 major bugs with corresponding results and reasons.\n\nESA Ariane5 explosion: Loss of over 300mil dollars and research time\n\nWhy: function reused from 4th project in the 5th project with assumption and without examination. Backup system failed to work.\nLesson : we use software package for multiple projects without examining requirements.\n\nAT&T Long distance: simple update right before release triggered malfunction remedy and kept being restarted\n\nWhy: “self healing” feature added without understanding the entire system and ramification\nLesson: (sunnys note) documentation is important for collaborators\n\nPatrio sysstem failure: Ground to air missle to intercept in-coming missle failed and 28 killed and hundreds injured\n\nWhy: calculation of location failed due to not applying subroutine. Some error occurred but could not be replicated thus ignored.\nLesson: when encounter errors, don’t just ignore.\nfind out what causes it. Test any edge case. Human intervention and supervision when needed.\n\n\n\n\n\nTalk was about the concept of introspection in python and functions to examine.\n(sunny) It may be a good practice to write a set of important methods for introspection.\n\nhelp() It is used it to find what other functions do\nhasattr() Checks if an object has an attribute\ngetattr() Returns the contents of an attribute if there are some.\nrepr() Return string representation of object\ncallable() Checks if an object is a callable object (a function)or not.\nissubclass() Checks if a specific class is a derived class of another class.\nisinstance() Checks if an objects is an instance of a specific class.\nsys() Give access to system specific variables and functions\n__doc__ Return some documentation about an object\n__name__ Return the name of the object.\n\n\n\n\n\njupyternotebook as serverless\nexample shows a web service deployed on fastAPI framework on Azure\n\n\n\n\n\nMLOps life cycle includes data prep, model train, evaluation, deployment, monitoring, and maintenance.\nWhy use MLOps?\n\nto speed deployment process\nto enhance collaboration\nto make it scalable\nto support governance\nfor continuous training\n\nChallenges faced with just model versioning to track and manage over time\n\ncomponents: artifact, version\nstorage overhead\ncomplex dependencies\nData issues: when data drift occurs, there’s a change in distribution of model based on input data, degrading performance\n\nDetection methods - Stat test/Performance monitoring/Visualization\n\nPerformance monitoring\n\nCatch issues early\nCreate metrics with accuracy, latency/throughput, business metrics, user engagement, etc.\n\nSet alert notificiation\nCreate a dashboard\n\nContinuous Training\n\nRegular updatres of model to adapt to new data patterns\nAuto retraining/incremental learning\n\nCollaboration - in all levels (data science, data/software engineering, devOps)\nBest practices\n\nStart small to scale\nEmbrace experiment\nFocus on quality data\nSecurity and compliance\nContinuous monitoring/retrain\nRight tools\n\n\n\n\n\nPresenter demonstrated creation of pygame and how easy it can be to create a quick and simple graphic with a moving object using pygame package.\n\n\n\n\n\n\nTODO\n\n\n\nMore talks to cover\n\nproblem solving by dissolution (Grothendieck and Jean-Pierre Serre, author of A Course in Arithmatic in pdf)\nmore on dunder\ntesting with playwright\npython & rust\npyscript\nusing K-means to play with the colors of photos\nlightning talks (nasa earth data, quarto from posit developer)\nGraphQL Operations\nReproducibility with and without docker\nAPI + cli\nNotably inaccessible (Jupyter)\ncircuit python (microcontroller)\ncontainerizing python\nCircuitPython for microcontrollers\nsprint (py opensci)\n\nOther topics\n\nhttps://temporal.io. perhaps solution for error handling when requesting data through erddap? open source/can be used with php, python, java, etc.\ncasual talk with fellow attendee on github rebase and grouping commits and random forest algorithm"
  },
  {
    "objectID": "posts/blog-post/pycascade24.html#pycascade-2024",
    "href": "posts/blog-post/pycascade24.html#pycascade-2024",
    "title": "PyCascades 2024",
    "section": "",
    "text": "Typing concepts/categories\n\nstrongly vs weakly (“loosely”)\ndynamically vs statically\nDuck typing and use of dunder\n\nPython best practices\n\nPython likes protocols/strongly type (type hinting/duck typing)\nAnnotate it as concrete as possible\nDeclarative data Types\n\n    def func() -&gt; list:  \n    def func() -&gt; list[str]:\n\nPermissive input types and strict return types\nUse standard libraries\nIf you are creating for publishing for other people, more typed the better\nBut.. if it doesn’t work for you, don’t do it.\n\n\n\n\n\nFrom bugs, results and analysis of the bugs, we can learn so much and improve coding practices. The speaker presented 3 major bugs with corresponding results and reasons.\n\nESA Ariane5 explosion: Loss of over 300mil dollars and research time\n\nWhy: function reused from 4th project in the 5th project with assumption and without examination. Backup system failed to work.\nLesson : we use software package for multiple projects without examining requirements.\n\nAT&T Long distance: simple update right before release triggered malfunction remedy and kept being restarted\n\nWhy: “self healing” feature added without understanding the entire system and ramification\nLesson: (sunnys note) documentation is important for collaborators\n\nPatrio sysstem failure: Ground to air missle to intercept in-coming missle failed and 28 killed and hundreds injured\n\nWhy: calculation of location failed due to not applying subroutine. Some error occurred but could not be replicated thus ignored.\nLesson: when encounter errors, don’t just ignore.\nfind out what causes it. Test any edge case. Human intervention and supervision when needed.\n\n\n\n\n\nTalk was about the concept of introspection in python and functions to examine.\n(sunny) It may be a good practice to write a set of important methods for introspection.\n\nhelp() It is used it to find what other functions do\nhasattr() Checks if an object has an attribute\ngetattr() Returns the contents of an attribute if there are some.\nrepr() Return string representation of object\ncallable() Checks if an object is a callable object (a function)or not.\nissubclass() Checks if a specific class is a derived class of another class.\nisinstance() Checks if an objects is an instance of a specific class.\nsys() Give access to system specific variables and functions\n__doc__ Return some documentation about an object\n__name__ Return the name of the object.\n\n\n\n\n\njupyternotebook as serverless\nexample shows a web service deployed on fastAPI framework on Azure\n\n\n\n\n\nMLOps life cycle includes data prep, model train, evaluation, deployment, monitoring, and maintenance.\nWhy use MLOps?\n\nto speed deployment process\nto enhance collaboration\nto make it scalable\nto support governance\nfor continuous training\n\nChallenges faced with just model versioning to track and manage over time\n\ncomponents: artifact, version\nstorage overhead\ncomplex dependencies\nData issues: when data drift occurs, there’s a change in distribution of model based on input data, degrading performance\n\nDetection methods - Stat test/Performance monitoring/Visualization\n\nPerformance monitoring\n\nCatch issues early\nCreate metrics with accuracy, latency/throughput, business metrics, user engagement, etc.\n\nSet alert notificiation\nCreate a dashboard\n\nContinuous Training\n\nRegular updatres of model to adapt to new data patterns\nAuto retraining/incremental learning\n\nCollaboration - in all levels (data science, data/software engineering, devOps)\nBest practices\n\nStart small to scale\nEmbrace experiment\nFocus on quality data\nSecurity and compliance\nContinuous monitoring/retrain\nRight tools\n\n\n\n\n\nPresenter demonstrated creation of pygame and how easy it can be to create a quick and simple graphic with a moving object using pygame package.\n\n\n\n\n\n\nTODO\n\n\n\nMore talks to cover\n\nproblem solving by dissolution (Grothendieck and Jean-Pierre Serre, author of A Course in Arithmatic in pdf)\nmore on dunder\ntesting with playwright\npython & rust\npyscript\nusing K-means to play with the colors of photos\nlightning talks (nasa earth data, quarto from posit developer)\nGraphQL Operations\nReproducibility with and without docker\nAPI + cli\nNotably inaccessible (Jupyter)\ncircuit python (microcontroller)\ncontainerizing python\nCircuitPython for microcontrollers\nsprint (py opensci)\n\nOther topics\n\nhttps://temporal.io. perhaps solution for error handling when requesting data through erddap? open source/can be used with php, python, java, etc.\ncasual talk with fellow attendee on github rebase and grouping commits and random forest algorithm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "JS Docs\n\n\n\n\n\n\ndocumentation\n\n\njavascript\n\n\n\nHow I use it to organize and view existing javascript files\n\n\n\n\n\nSep 6, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPosit:Conf 2024\n\n\n\n\n\n\nworkshops\n\n\nopenscience\n\n\npython\n\n\nR\n\n\n\nWhat I’ve learned from Posit Conference\n\n\n\n\n\nAug 14, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPyCascades 2024\n\n\n\n\n\n\nworkshops\n\n\nopenscience\n\n\npython\n\n\n\nPersonal notes on the presentations and lessons learned\n\n\n\n\n\nApr 10, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nSet up JupyterHub on AWS\n\n\n\n\n\n\nopenscience\n\n\ncloud\n\n\n\nHow to set up JupyterHub on AWS (Amazon web services)\n\n\n\n\n\nApr 3, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\n{nco} cheatsheet\n\n\n\n\n\n\nnetcdf\n\n\nnco\n\n\n\nWorking with and nco tool\n\n\n\n\n\nApr 2, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nLit Review: Comparing resampling methods\n\n\n\n\n\n\nmachine learning\n\n\nliterature review\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog-post/jsdocs.html",
    "href": "posts/blog-post/jsdocs.html",
    "title": "JS Docs",
    "section": "",
    "text": "I find javascript files difficult to read and follow especially when they were written by someone else and your job is to modify or fix what isn’t working. Often the script contains thousands of lines and it’s unclear which objects the javascript actions are applied to.\nThere are some good comments in the scripts but not in a consistent format recommended by standards. In this situation, I find it easy to first generate web-based documents using JSDoc and start from there.\nSince I don’t the admin access, I downloaded and installed locally\nnpm install jsdoc\njsdocs myjavascript.js\nThis created a webpage reference to all functions with any comments or annotations."
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html",
    "href": "posts/blog-post/settingup_jh_aws.html",
    "title": "Set up JupyterHub on AWS",
    "section": "",
    "text": "The set up instruction is based on the https://saturncloud.io/blog/jupyterhub_aws/\n\n\n\n\n\nScalable JupyterHub with RStudio and Python\nPre-installation of packages\nCompute power sufficient for CoastWatch R and Python tutorials\n\n\n\n\n\nVPC (Virtual Private Cloud) - creating a private network\n\nCloud Formation - creating and starting aws services using a pre-specified template\nCloud9 - accessing aws services via command line (or download and use aws cli)\nIAM Role - creating roles/users and associated policies\nEKS (Elastic Kubernetes Service) - creating EKS cluster and manage kubernetes control plane\nEBS (Elastic Block Storage) - creating storage for software/data installed on the cluster\n\n\n\n\n\n\n\n\n\n\n\nAWS Account\n\n\n\nThe process of creating AWS account is omitted in this document.\n\n\n\n\n\nDownload VPC template from the satern cloud website, or amazon-eks-vpc-private-subnets.yaml\nGo to AWS Cloud Formation Console and create VPC using the downloaded template\n\n\n\n\n\nEither in Cloud9 or your local terminal\n\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nJupyterhub didn’t work with lower version of helm.\n\n\ncurl https://raw.githubusercontent.com/helm/helm/HEAD/scripts/get-helm-3 | bash\nhelm version\n\nhelm upgrade --cleanup-on-fail \\\n--install YOUR_RELEASE jupyterhub/jupyterhub \\\n--namespace YOUR_NS \\\n--create-namespace --version='3.2.0' \n--values config.yaml \\\n\n\n\n\nGo to IAM (Identity Access Management) Console\nCreate IAM Role\n\nSelect Usercase: eksCluster, Name: my-eks-role\nFor policies:\n\nUsercase: EC2 and attach the following:\n\nAmazonEKSWorkerNodePolicy\nAmazonEC2ContainerRegistryReadOnly\nAmazonEKS_CNI_Policy\n\n\n\nCreate IAM User\n\nSelect Attach policies directly\nUsername : my-user; Select AdminAccess\nDownload Access key to your desktop\n\n\n\n\n\n\nGo to Cluster (EKS) Console and Create EKS cluster\n\nuse default values for data entry\nCluster servic role - eks role created\nCluster authentication mode: select EKS API and ConfigMap\nSelect Your VPC and Public and private endpoint\nLeave security group empty\nEndpoint access - public and private\n\n\n\n\n\n\nSelect “Compute” from the cluster page\nSelect add node role created\n\nNode IAM role: node group\n\nselect instance type for a node\n\nAMI type: amazone linux\non-demand\nt3.medium\ndisk size: 20GiB\nnode size (desired, min and max)\n\n\n\n\n\nThis step is to set up IAM role and user for cluster access\n\nFrom the cluster created, copy OpenID Connect provider URL from Cluster\nGo to IAM Console\nSelect Identity Providers under IAM and OpenID Connect and copy the URL\n\nAudience: sts.amazonaws.com\nStorage Configuration, Access -&gt; Configure IAM access entry -&gt; Select my-user (whatever you set up as iam user)\nIAM principal ARN\nAudience: sts.amazonaws.com\n\n\n\n\n\n\nGo to the IAM Console\nCreate IAM role (ebs-role)\nSelect Web Identity\nSelect aws add and sts.amazonaws\nSelect AmazonEBSCSIDriverPolicy, Amazon EBS CSI Driver operator\nGo to Add-On and add AmazonEBS\n\nAccess : select my-user, Policy: EKSClusterAdmin\n\n\n\n\n\n\nGo to EKS Cluster Console\nMake all values node=1 and max to =2 (up to your requirement)\nSelect EC2Role\n\n\n\n\n\nGo to EFS Console\nSelect your VPC\nCreate\n\n\n\n\n\nClick on Addon on your Cluster\nClick on more Addon and select Amazon EBS CSI Driver Info\nSelect ebs-role\n\n\n\n\n\nIn your terminal or Cloud9, add ACCESS information downloaded earlier.\n\nexport AWS_ACCESS_KEY_ID=[ADD YOURS]\nexport AWS_SECRET_ACCESS_KEY=[ADD YOURS]\nexport AWS_DEFAULT_REGION=[YOUR REGION]\n\naws sts get-caller-identity\naws eks update-kubeconfig --region your_region --name name_of_your_cluster\nkubectl get svc\nKubectl get node\n\n\n\n\nhelm upgrade --cleanup-on-fail \\\n--install noaa-release jupyterhub/jupyterhub \\\n--namespace noaa \\\n--create-namespace \\\n--version='3.2.0' \\\n--values config_basic_docker.yaml \\\n--debug\n\n\n\n\nkubectl --namespace your_namespace get service proxy-public"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#requirements",
    "href": "posts/blog-post/settingup_jh_aws.html#requirements",
    "title": "Set up JupyterHub on AWS",
    "section": "",
    "text": "Scalable JupyterHub with RStudio and Python\nPre-installation of packages\nCompute power sufficient for CoastWatch R and Python tutorials\n\n\n\n\n\nVPC (Virtual Private Cloud) - creating a private network\n\nCloud Formation - creating and starting aws services using a pre-specified template\nCloud9 - accessing aws services via command line (or download and use aws cli)\nIAM Role - creating roles/users and associated policies\nEKS (Elastic Kubernetes Service) - creating EKS cluster and manage kubernetes control plane\nEBS (Elastic Block Storage) - creating storage for software/data installed on the cluster"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#set-up-aws-architecture",
    "href": "posts/blog-post/settingup_jh_aws.html#set-up-aws-architecture",
    "title": "Set up JupyterHub on AWS",
    "section": "",
    "text": "AWS Account\n\n\n\nThe process of creating AWS account is omitted in this document.\n\n\n\n\n\nDownload VPC template from the satern cloud website, or amazon-eks-vpc-private-subnets.yaml\nGo to AWS Cloud Formation Console and create VPC using the downloaded template\n\n\n\n\n\nEither in Cloud9 or your local terminal\n\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nJupyterhub didn’t work with lower version of helm.\n\n\ncurl https://raw.githubusercontent.com/helm/helm/HEAD/scripts/get-helm-3 | bash\nhelm version\n\nhelm upgrade --cleanup-on-fail \\\n--install YOUR_RELEASE jupyterhub/jupyterhub \\\n--namespace YOUR_NS \\\n--create-namespace --version='3.2.0' \n--values config.yaml \\\n\n\n\n\nGo to IAM (Identity Access Management) Console\nCreate IAM Role\n\nSelect Usercase: eksCluster, Name: my-eks-role\nFor policies:\n\nUsercase: EC2 and attach the following:\n\nAmazonEKSWorkerNodePolicy\nAmazonEC2ContainerRegistryReadOnly\nAmazonEKS_CNI_Policy\n\n\n\nCreate IAM User\n\nSelect Attach policies directly\nUsername : my-user; Select AdminAccess\nDownload Access key to your desktop\n\n\n\n\n\n\nGo to Cluster (EKS) Console and Create EKS cluster\n\nuse default values for data entry\nCluster servic role - eks role created\nCluster authentication mode: select EKS API and ConfigMap\nSelect Your VPC and Public and private endpoint\nLeave security group empty\nEndpoint access - public and private\n\n\n\n\n\n\nSelect “Compute” from the cluster page\nSelect add node role created\n\nNode IAM role: node group\n\nselect instance type for a node\n\nAMI type: amazone linux\non-demand\nt3.medium\ndisk size: 20GiB\nnode size (desired, min and max)\n\n\n\n\n\nThis step is to set up IAM role and user for cluster access\n\nFrom the cluster created, copy OpenID Connect provider URL from Cluster\nGo to IAM Console\nSelect Identity Providers under IAM and OpenID Connect and copy the URL\n\nAudience: sts.amazonaws.com\nStorage Configuration, Access -&gt; Configure IAM access entry -&gt; Select my-user (whatever you set up as iam user)\nIAM principal ARN\nAudience: sts.amazonaws.com\n\n\n\n\n\n\nGo to the IAM Console\nCreate IAM role (ebs-role)\nSelect Web Identity\nSelect aws add and sts.amazonaws\nSelect AmazonEBSCSIDriverPolicy, Amazon EBS CSI Driver operator\nGo to Add-On and add AmazonEBS\n\nAccess : select my-user, Policy: EKSClusterAdmin\n\n\n\n\n\n\nGo to EKS Cluster Console\nMake all values node=1 and max to =2 (up to your requirement)\nSelect EC2Role\n\n\n\n\n\nGo to EFS Console\nSelect your VPC\nCreate\n\n\n\n\n\nClick on Addon on your Cluster\nClick on more Addon and select Amazon EBS CSI Driver Info\nSelect ebs-role\n\n\n\n\n\nIn your terminal or Cloud9, add ACCESS information downloaded earlier.\n\nexport AWS_ACCESS_KEY_ID=[ADD YOURS]\nexport AWS_SECRET_ACCESS_KEY=[ADD YOURS]\nexport AWS_DEFAULT_REGION=[YOUR REGION]\n\naws sts get-caller-identity\naws eks update-kubeconfig --region your_region --name name_of_your_cluster\nkubectl get svc\nKubectl get node\n\n\n\n\nhelm upgrade --cleanup-on-fail \\\n--install noaa-release jupyterhub/jupyterhub \\\n--namespace noaa \\\n--create-namespace \\\n--version='3.2.0' \\\n--values config_basic_docker.yaml \\\n--debug\n\n\n\n\nkubectl --namespace your_namespace get service proxy-public"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#delete-eks-cluster",
    "href": "posts/blog-post/settingup_jh_aws.html#delete-eks-cluster",
    "title": "Set up JupyterHub on AWS",
    "section": "Delete EKS Cluster",
    "text": "Delete EKS Cluster\n\ndelete cluster node group first\ndelete eks cluster\n\n\n\n\n\n\n\nNote\n\n\n\nEBS should be deleted as cluster is terminated.\nTODO: verify"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#delete-efs",
    "href": "posts/blog-post/settingup_jh_aws.html#delete-efs",
    "title": "Set up JupyterHub on AWS",
    "section": "Delete EFS",
    "text": "Delete EFS\n\nGo to EFS file system and Delete EFS"
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html",
    "href": "posts/blog-post/posit-conf24.html",
    "title": "Posit:Conf 2024",
    "section": "",
    "text": "This was my first Posit Conference, and I was eager to learn about new technologies and tools. Interestingly, what I enjoyed most were the keynote speakers and the spontaneous conversations during breaks.\nThe keynote speakers were inspirational, offering a big-picture perspective and motivations for pursuing a meaningful career path. The impromptu discussions with conference attendees—who came from diverse backgrounds gave me valuable insights, not just in data science, but in life.\nHaving had this wonderful opportunity to attend, I’d like to share a few key takeways. This blog isn’t about the specific topics covered in the conference session, but rather about what I took away from the overall experience."
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#keynote-take-away",
    "href": "posts/blog-post/posit-conf24.html#keynote-take-away",
    "title": "Posit:Conf 2024",
    "section": "Keynote take-away",
    "text": "Keynote take-away\nAt times work can settle into comfortable routines or become a pursuit of personal intellectual satisfaction.\nWhile goals and objectives guide our work and shape our workflows, I often reflect on what the work truly means to me.\nSome of the keynote messages resonated with me as questions i can ask myself.\n\nHow can we be innovative to make things better?\nWhatever I do, aim to contribute–even in small ways–to creating positive change.\nCreate Virtuous cycles to benefit everyone involved. Think of ways to reinvest my benefits back into the community\nOpen Science: Use data to understand the world better. Then we know how to make it better\nNegative bias is a serious threat to our well-being and ability to address the problems we face\nWorking with people from different background can be challenging.\nAgree on a set of core values such as radical transparency, trust, respect, courage, commitment"
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#sessions-conversation-take-away",
    "href": "posts/blog-post/posit-conf24.html#sessions-conversation-take-away",
    "title": "Posit:Conf 2024",
    "section": "Sessions & Conversation take-away",
    "text": "Sessions & Conversation take-away\n\nWorking with people from different backgrounds can be challenging. It’s important to agree on a set of core values, such as radical transparency, trust, respect, courage, and commitment.\nChatGPT is becoming widely used, especially among the new generation of professionals. Traditional software engineering interviews often involve requesting code samples, but this practice is becoming less relevant as many candidates now use ChatGPT. What does the future hold? Perhaps it lies with those who can construct the “right” questions to solve difficult problems."
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#key-statiscal-and-machine-learning-tools",
    "href": "posts/blog-post/posit-conf24.html#key-statiscal-and-machine-learning-tools",
    "title": "Posit:Conf 2024",
    "section": "Key Statiscal and Machine Learning tools",
    "text": "Key Statiscal and Machine Learning tools\n\n{marginaleffects} package for model comparisons. you write a comparison function and the package help to generate ggplot and other output to compare models.\nPrediction Intervals: github.com/brshallo/posit-2024, talked about different ways to construct and evaluate prediction intervals\n\nInterval width is widely used and good to evaluate if a model is improving as the width becomes narrow. (Constant variance desired)\nin Conformed prediction, the assumption is relaxed, and you compute upper and lower bands. (manokhin molar). Use train, calibration, and test datasets for estimating intervals.\n\nDeep learning: {keras3} allows the use of keras and tensorflow in python throough reticulate.\n\ncreate neural network architecture just like python keras.\noffers the interface to GPU.\nsave the models in *.keras to change the backend for different cycles. (save and read model)\ndistributed training\nkeras3.posit.co\n\n\n*Avoid garbage in/garbage out: Use interactive tool like shiny for data cleaning"
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#tools-to-check-out",
    "href": "posts/blog-post/posit-conf24.html#tools-to-check-out",
    "title": "Posit:Conf 2024",
    "section": "Tools to check out",
    "text": "Tools to check out\n\nWeb/Report publishing : Quarto Dashboard for web dashboard and quarto PDF using typst (faster than latex)\nEducation/Training : WebR/Quarto live for interactive web interface with programming language, ideal for training. It can be either a standalone web application or a code chunk in a quarto document.\nCloud Computing: Posit Workbench is a cloud computing and it handles infrastrcuture manageement and security requirements. It can be deployed in Posit:Connect.\nCloud and Collaboration: Github codespace for computing and sharing simultaneously and is free for # of hours and you can use docker image to set a virtual environment.\n{pins}: customizing and automating report generation and delivery"
  },
  {
    "objectID": "posts/blog-post/nco_cheatsheet.html",
    "href": "posts/blog-post/nco_cheatsheet.html",
    "title": "{nco} cheatsheet",
    "section": "",
    "text": "This is an instruction to create a netcdf file with templated metadata.\n\nCreate a metadata template from an existing nc file\nncdump samplefile.nc &gt; metadata_header.txt\nncks -A -v temperature newdata.nc newfile.nc\nncdump -v temperature newfile.nc\nncks -A newdata.nc newfile.nc"
  }
]