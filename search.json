[
  {
    "objectID": "posts/blog-post/index.html",
    "href": "posts/blog-post/index.html",
    "title": "Lit Review: Comparing resampling methods",
    "section": "",
    "text": "Article: Validation of machine learning ridge regression models using Monte Carlo, bootstrap, and variations in cross-validation Link to the article\nThe article compares performance of the commonly used resampling techniques in machine/statistical learning using ridge regression models. There are many conflicting suggestions for methods and parameter values for fine tuning hyper parameters.\nUsing simulations, the author tries to answer the following questions:\n\nWhich of the four resampling methods is most effective in selecting a suitable regulatiziation paremter \\(\\lambda\\)?\nDoes increasing the number of repetitions - from 10 to 50 - improve the performance of the resampling method?\nKeeping the number of repetitions constant, which approach, single-run cross-validation or repated cross-validation, performs better?\nFor k=fold CV, what is an appropriate fold size k?\nWhich randomization approach is more effective, Monte Carlo (sampling without replacement) or bootstrap (with replacement)?\n\n\nRidge regression model\nIn regression model where Y is the target (response) variable and X_1, X_2, ..XP are feature (independent) variables, our goal is to find _beta coefficients that minimize the SSE in OLS. Linear model could have an overfitting problem.\nTo deal with overfitting issue in linear models, the ridge regression model was used to apply regularization. The penality term was added to the SSE equation.\n\\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} ({\\beta}_j^2)\n\\]\nThe ridge regression model and MSE are used to find the lambda estimate. Data set is split into a training and validation sets where training set is used for parameter estimation and the validation set for evaluating overfit. One split of a dataset could create bias in MSE thus resampling methods are used to obtain MSE estimates.\nRegularization results in shiriking the \\(\\beta_j\\) toward 0 as \\(\\lambda\\) becomes large (or \\(\\beta\\) is same when \\(\\lambda\\) =0).\n\n\nResampling methods\n\nMonte Carlo\n(Other names: repeated learning-testing, repeated holdout, random subsampling)\nMonte Carlo sampling randomly shuffles the dataset (randomizing rows) and select the first 75% to be a training set and the remaining 25% as a validation set. This is sampling without replacement.\nThe validation error is averaged over n repetitions.\n\n\nBootstrap\nSimilar to Monte Carlo sampling where data are randomly selected from the dataset but with replacement.\nThe validation error is averaged over n repetitions.\n\n\n\n\n\n\nSunnys note\n\n\n\ncommonly (or by default) equal sampling probability?\n\n\n\n\nk-fold CV (Cross-validation)\nA dataset is divided into k partition at random (most commonly k=5 or 10). One partition is held out (set aside) as a validation set, and the remaining k-1 partitions of data are used as a training set. __The validation error_ is averaged over k repetitions.\nMost extreme case of k-fold CV, known as LOOCV (Leave One Out CV) is k = n where 1 observation is held out at each iteration.\n\n\nrepeated k-fold CV\nk-fold CV is done over n repetition and obtain \\({n} x {k}\\) validation errors.\nThe validation error is averaged over \\({n} \\times {k}\\)\n\n\n\n\n\n\nNote\n\n\n\nmost commonly use k=fold CV only once (rep =1) and repeated method is suggested as a way of obtaining more accurate and reliable estimates of error rates"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is a space where I add notes about something I learn."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Lit Review: Comparing resampling methods\n\n\n\n\n\n\nmachine learning\n\n\nliterature review\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\nNo matching items"
  }
]