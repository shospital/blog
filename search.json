[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is a space where I add notes about something I learn."
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html",
    "href": "posts/blog-post/settingup_jh_aws.html",
    "title": "Set up JupyterHub on AWS",
    "section": "",
    "text": "The set up instruction is based on the https://saturncloud.io/blog/jupyterhub_aws/\n\n\n\n\n\nScalable JupyterHub with RStudio and Python\nPre-installation of packages\nCompute power sufficient for CoastWatch R and Python tutorials\n\n\n\n\n\nVPC (Virtual Private Cloud) - creating a private network\n\nCloud Formation - creating and starting aws services using a pre-specified template\nCloud9 - accessing aws services via command line (or download and use aws cli)\nIAM Role - creating roles/users and associated policies\nEKS (Elastic Kubernetes Service) - creating EKS cluster and manage kubernetes control plane\nEBS (Elastic Block Storage) - creating storage for software/data installed on the cluster\n\n\n\n\n\n\n\n\n\n\n\nAWS Account\n\n\n\nThe process of creating AWS account is omitted in this document.\n\n\n\n\n\nDownload VPC template from the satern cloud website, or amazon-eks-vpc-private-subnets.yaml\nGo to AWS Cloud Formation Console and create VPC using the downloaded template\n\n\n\n\n\nEither in Cloud9 or your local terminal\n\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nJupyterhub didn’t work with lower version of helm.\n\n\ncurl https://raw.githubusercontent.com/helm/helm/HEAD/scripts/get-helm-3 | bash\nhelm version\n\nhelm upgrade --cleanup-on-fail \\\n--install YOUR_RELEASE jupyterhub/jupyterhub \\\n--namespace YOUR_NS \\\n--create-namespace --version='3.2.0' \n--values config.yaml \\\n\n\n\n\nGo to IAM (Identity Access Management) Console\nCreate IAM Role\n\nSelect Usercase: eksCluster, Name: my-eks-role\nFor policies:\n\nUsercase: EC2 and attach the following:\n\nAmazonEKSWorkerNodePolicy\nAmazonEC2ContainerRegistryReadOnly\nAmazonEKS_CNI_Policy\n\n\n\nCreate IAM User\n\nSelect Attach policies directly\nUsername : my-user; Select AdminAccess\nDownload Access key to your desktop\n\n\n\n\n\n\nGo to Cluster (EKS) Console and Create EKS cluster\n\nuse default values for data entry\nCluster servic role - eks role created\nCluster authentication mode: select EKS API and ConfigMap\nSelect Your VPC and Public and private endpoint\nLeave security group empty\nEndpoint access - public and private\n\n\n\n\n\n\nSelect “Compute” from the cluster page\nSelect add node role created\n\nNode IAM role: node group\n\nselect instance type for a node\n\nAMI type: amazone linux\non-demand\nt3.medium\ndisk size: 20GiB\nnode size (desired, min and max)\n\n\n\n\n\nThis step is to set up IAM role and user for cluster access\n\nFrom the cluster created, copy OpenID Connect provider URL from Cluster\nGo to IAM Console\nSelect Identity Providers under IAM and OpenID Connect and copy the URL\n\nAudience: sts.amazonaws.com\nStorage Configuration, Access -&gt; Configure IAM access entry -&gt; Select my-user (whatever you set up as iam user)\nIAM principal ARN\nAudience: sts.amazonaws.com\n\n\n\n\n\n\nGo to the IAM Console\nCreate IAM role (ebs-role)\nSelect Web Identity\nSelect aws add and sts.amazonaws\nSelect AmazonEBSCSIDriverPolicy, Amazon EBS CSI Driver operator\nGo to Add-On and add AmazonEBS\n\nAccess : select my-user, Policy: EKSClusterAdmin\n\n\n\n\n\n\nGo to EKS Cluster Console\nMake all values node=1 and max to =2 (up to your requirement)\nSelect EC2Role\n\n\n\n\n\nGo to EFS Console\nSelect your VPC\nCreate\n\n\n\n\n\nClick on Addon on your Cluster\nClick on more Addon and select Amazon EBS CSI Driver Info\nSelect ebs-role\n\n\n\n\n\nIn your terminal or Cloud9, add ACCESS information downloaded earlier.\n\nexport AWS_ACCESS_KEY_ID=[ADD YOURS]\nexport AWS_SECRET_ACCESS_KEY=[ADD YOURS]\nexport AWS_DEFAULT_REGION=[YOUR REGION]\n\naws sts get-caller-identity\naws eks update-kubeconfig --region your_region --name name_of_your_cluster\nkubectl get svc\nKubectl get node\n\n\n\n\nhelm upgrade --cleanup-on-fail \\\n--install noaa-release jupyterhub/jupyterhub \\\n--namespace noaa \\\n--create-namespace \\\n--version='3.2.0' \\\n--values config_basic_docker.yaml \\\n--debug\n\n\n\n\nkubectl --namespace your_namespace get service proxy-public"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#requirements",
    "href": "posts/blog-post/settingup_jh_aws.html#requirements",
    "title": "Set up JupyterHub on AWS",
    "section": "",
    "text": "Scalable JupyterHub with RStudio and Python\nPre-installation of packages\nCompute power sufficient for CoastWatch R and Python tutorials\n\n\n\n\n\nVPC (Virtual Private Cloud) - creating a private network\n\nCloud Formation - creating and starting aws services using a pre-specified template\nCloud9 - accessing aws services via command line (or download and use aws cli)\nIAM Role - creating roles/users and associated policies\nEKS (Elastic Kubernetes Service) - creating EKS cluster and manage kubernetes control plane\nEBS (Elastic Block Storage) - creating storage for software/data installed on the cluster"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#set-up-aws-architecture",
    "href": "posts/blog-post/settingup_jh_aws.html#set-up-aws-architecture",
    "title": "Set up JupyterHub on AWS",
    "section": "",
    "text": "AWS Account\n\n\n\nThe process of creating AWS account is omitted in this document.\n\n\n\n\n\nDownload VPC template from the satern cloud website, or amazon-eks-vpc-private-subnets.yaml\nGo to AWS Cloud Formation Console and create VPC using the downloaded template\n\n\n\n\n\nEither in Cloud9 or your local terminal\n\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nJupyterhub didn’t work with lower version of helm.\n\n\ncurl https://raw.githubusercontent.com/helm/helm/HEAD/scripts/get-helm-3 | bash\nhelm version\n\nhelm upgrade --cleanup-on-fail \\\n--install YOUR_RELEASE jupyterhub/jupyterhub \\\n--namespace YOUR_NS \\\n--create-namespace --version='3.2.0' \n--values config.yaml \\\n\n\n\n\nGo to IAM (Identity Access Management) Console\nCreate IAM Role\n\nSelect Usercase: eksCluster, Name: my-eks-role\nFor policies:\n\nUsercase: EC2 and attach the following:\n\nAmazonEKSWorkerNodePolicy\nAmazonEC2ContainerRegistryReadOnly\nAmazonEKS_CNI_Policy\n\n\n\nCreate IAM User\n\nSelect Attach policies directly\nUsername : my-user; Select AdminAccess\nDownload Access key to your desktop\n\n\n\n\n\n\nGo to Cluster (EKS) Console and Create EKS cluster\n\nuse default values for data entry\nCluster servic role - eks role created\nCluster authentication mode: select EKS API and ConfigMap\nSelect Your VPC and Public and private endpoint\nLeave security group empty\nEndpoint access - public and private\n\n\n\n\n\n\nSelect “Compute” from the cluster page\nSelect add node role created\n\nNode IAM role: node group\n\nselect instance type for a node\n\nAMI type: amazone linux\non-demand\nt3.medium\ndisk size: 20GiB\nnode size (desired, min and max)\n\n\n\n\n\nThis step is to set up IAM role and user for cluster access\n\nFrom the cluster created, copy OpenID Connect provider URL from Cluster\nGo to IAM Console\nSelect Identity Providers under IAM and OpenID Connect and copy the URL\n\nAudience: sts.amazonaws.com\nStorage Configuration, Access -&gt; Configure IAM access entry -&gt; Select my-user (whatever you set up as iam user)\nIAM principal ARN\nAudience: sts.amazonaws.com\n\n\n\n\n\n\nGo to the IAM Console\nCreate IAM role (ebs-role)\nSelect Web Identity\nSelect aws add and sts.amazonaws\nSelect AmazonEBSCSIDriverPolicy, Amazon EBS CSI Driver operator\nGo to Add-On and add AmazonEBS\n\nAccess : select my-user, Policy: EKSClusterAdmin\n\n\n\n\n\n\nGo to EKS Cluster Console\nMake all values node=1 and max to =2 (up to your requirement)\nSelect EC2Role\n\n\n\n\n\nGo to EFS Console\nSelect your VPC\nCreate\n\n\n\n\n\nClick on Addon on your Cluster\nClick on more Addon and select Amazon EBS CSI Driver Info\nSelect ebs-role\n\n\n\n\n\nIn your terminal or Cloud9, add ACCESS information downloaded earlier.\n\nexport AWS_ACCESS_KEY_ID=[ADD YOURS]\nexport AWS_SECRET_ACCESS_KEY=[ADD YOURS]\nexport AWS_DEFAULT_REGION=[YOUR REGION]\n\naws sts get-caller-identity\naws eks update-kubeconfig --region your_region --name name_of_your_cluster\nkubectl get svc\nKubectl get node\n\n\n\n\nhelm upgrade --cleanup-on-fail \\\n--install noaa-release jupyterhub/jupyterhub \\\n--namespace noaa \\\n--create-namespace \\\n--version='3.2.0' \\\n--values config_basic_docker.yaml \\\n--debug\n\n\n\n\nkubectl --namespace your_namespace get service proxy-public"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#delete-eks-cluster",
    "href": "posts/blog-post/settingup_jh_aws.html#delete-eks-cluster",
    "title": "Set up JupyterHub on AWS",
    "section": "Delete EKS Cluster",
    "text": "Delete EKS Cluster\n\ndelete cluster node group first\ndelete eks cluster\n\n\n\n\n\n\n\nNote\n\n\n\nEBS should be deleted as cluster is terminated.\nTODO: verify"
  },
  {
    "objectID": "posts/blog-post/settingup_jh_aws.html#delete-efs",
    "href": "posts/blog-post/settingup_jh_aws.html#delete-efs",
    "title": "Set up JupyterHub on AWS",
    "section": "Delete EFS",
    "text": "Delete EFS\n\nGo to EFS file system and Delete EFS"
  },
  {
    "objectID": "posts/blog-post/iterables.html",
    "href": "posts/blog-post/iterables.html",
    "title": "Python Iterators, Iterables, Asynchronous",
    "section": "",
    "text": "Summary Note from RealPython (https://realpython.com/python-iterators-iterables/)"
  },
  {
    "objectID": "posts/blog-post/iterables.html#create-iterators-using-the-iterator-protocol",
    "href": "posts/blog-post/iterables.html#create-iterators-using-the-iterator-protocol",
    "title": "Python Iterators, Iterables, Asynchronous",
    "section": "Create iterators using the iterator protocol",
    "text": "Create iterators using the iterator protocol\nWhat is Iterator in Python\nAn iterator is an object that allows you to iterate over a collection of data and returns one item at a time and keeps track of the current state.\nWhat Is the Python Iterator Protocol\nWhile there are iterator objects, you can also create iterator for your custom class by implementing two special methods called iterator protocol.\n.__iter__() to intialize the iterator and return an interator object (self)\n.__next__() to iterate over iterator and will return the next value and raise an except StopIteration for the end of the collection.\nTypes of Iterators\nIterators can be used to perform various tasks: iterating over a collection of data to\n\nreturn each item\nreturn transformed item\nreturn newly generated item\n\nIterator examples\n# Iterator example to return its own value\nclass MyIterator:\n    def __init__(self, sequence):\n        self._sequence = seequence\n        self._index = 0\n\n    def __iter__(self):\n        return self \n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else \n            raise StopIteeration \n\nfor item in MyIterator([1, 2, 3])\n    print(item)\n\n# Iterator example to return transformed value\nclass MySquare:\n\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self \n    \n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            square = self._sequence[self._index] ** 2\n            self._index += 1\n            return square \n        else:\n            raise StopIteration\n\n\n# Iterator example to return generated value\nclass FibonacciIterator:\n    def __init__(self, stop=10):\n        self._stop = stop\n        self._index = 0\n        self._current = 0\n        self._next = 1\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; self._stop:\n            self._index += 1\n            fib_number = self._current\n            self._current, self._next = (\n                self._next,\n                self._current + self._next,\n            )\n            return fib_number\n        else:\n            raise StopIteration\nYou can create infinite iterator by skipping StopIteration"
  },
  {
    "objectID": "posts/blog-post/iterables.html#create-generator-iterators",
    "href": "posts/blog-post/iterables.html#create-generator-iterators",
    "title": "Python Iterators, Iterables, Asynchronous",
    "section": "Create Generator iterators",
    "text": "Create Generator iterators\nWhat is Generator Iterators\nA generator iterator is a function based iterator and must use yield. It’s simpler than class iterator.\nGenerator iterator expression is similar to the list comprehension but with parenthesis instead of brackets.\nSimilar to Iterator, Generator can also return item itself, transformed item, and new item\n# Generator example to return its own value\ndef myGeneratorIter(sequence):\n    for item in sequence:\n        yield(item)\n\nfor i in myGeneratorIter([1, 2, 3])\n    print(i)\n\n# Generator expression \n(item for item in [1, 2, 3]) # unlike list [], it uses ()\n\ngenExpression = (item for item in [1, 2, 3])\nfor i in genExpression:\n    print(i)\n\n\n# Generator example to return transformed value\ndef SquareGenerator(sequence):\n    for item in sequence:\n        yield(item**2)\n\nfor i in SquareGenerator([1,2,3])\n    print(i)"
  },
  {
    "objectID": "posts/blog-post/iterables.html#memory-efficient-data-processing",
    "href": "posts/blog-post/iterables.html#memory-efficient-data-processing",
    "title": "Python Iterators, Iterables, Asynchronous",
    "section": "Memory efficient data processing",
    "text": "Memory efficient data processing\nBenefits\n\nYou don’t need to store all the data in the computer memory at the same time.\nIt can decouple processing with data\nIterators are the only way to process infinite data streams\n\nRegular functions or comprehensions for data processing create data structure such as a list and it stores data in memory at the same time.\nIterators keep only one item in memory at a time, generating the next ones on demand or lazily.\nConstraints * You can’t iterate over an iterator. Once StopIteration is raised, the iterator is exhausted. * You can only move forward, not backyard. You only have __next__(), not previous. * unlike lists and tuples, iterators don’t allow indexing and slicing operations with the [] operator:\ndef square_list(sequence):\n    squares = []\n    for item in sequence:\n        squares.append(item**2)\n    return squares \n\nCreating pipeline with generator iterator\ndef to_square(numbers):\n    return (number**2 for number in numbers)\n\ndef to_cube(numbers):\n    return (number**3 for number in numbers)\n\ndef to_even(numbers):\n    return (number for number in numbers if number % 2 == 0)\n\ndef to_odd(numbers):\n    return (number for number in numbers if number % 2 != 0)\n\ndef to_string(numbers):\n    return (str(number) for number in numbers)\n\n&gt;&gt;&gt; import math_pipeline as mpl\n\n&gt;&gt;&gt; list(mpl.to_string(mpl.to_square(mpl.to_even(range(20)))))\n['0', '4', '16', '36', '64', '100', '144', '196', '256', '324']\n\n&gt;&gt;&gt; list(mpl.to_string(mpl.to_cube(mpl.to_odd(range(20)))))\n['1', '27', '125', '343', '729', '1331', '2197', '3375', '4913', '6859']"
  },
  {
    "objectID": "posts/blog-post/stac_aws.html",
    "href": "posts/blog-post/stac_aws.html",
    "title": "Setting up STAC on AWS",
    "section": "",
    "text": "=\n\nResources\n\nstac-fastapi\nEC2, ECS, or Lambda to run the API server.\nRDS PostgreSQL (with PostGIS) as the backend.\nS3 for storing the actual data files and/or static STAC JSONs (optional but common).\nLambda or Batch job to ingest new data and register it in the STAC API."
  },
  {
    "objectID": "posts/blog-post/index.html",
    "href": "posts/blog-post/index.html",
    "title": "Lit Review: Comparing resampling methods",
    "section": "",
    "text": "Article: Validation of machine learning ridge regression models using Monte Carlo, bootstrap, and variations in cross-validation Link to the article\nThe article compares performance of the commonly used resampling techniques in machine/statistical learning using ridge regression models. There are many conflicting suggestions for methods and parameter values for fine tuning hyper parameters.\nUsing simulations, the author tries to answer the following questions:\n\nWhich of the four resampling methods is most effective in selecting a suitable regulatiziation paremter \\(\\lambda\\)?\nDoes increasing the number of repetitions - from 10 to 50 - improve the performance of the resampling method?\nKeeping the number of repetitions constant, which approach, single-run cross-validation or repated cross-validation, performs better?\nFor k=fold CV, what is an appropriate fold size k?\nWhich randomization approach is more effective, Monte Carlo (sampling without replacement) or bootstrap (with replacement)?\n\n\nRidge regression model\nIn regression model where Y is the target (response) variable and X_1, X_2, ..XP are feature (independent) variables, our goal is to find _beta coefficients that minimize the SSE in OLS. Linear model could have an overfitting problem.\nTo deal with overfitting issue in linear models, the ridge regression model was used to apply regularization. The penality term was added to the SSE equation.\n\\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} ({\\beta}_j^2)\n\\]\nThe ridge regression model and MSE are used to find the lambda estimate. Data set is split into a training and validation sets where training set is used for parameter estimation and the validation set for evaluating overfit. One split of a dataset could create bias in MSE thus resampling methods are used to obtain MSE estimates.\nRegularization results in shiriking the \\(\\beta_j\\) toward 0 as \\(\\lambda\\) becomes large (or \\(\\beta\\) is same when \\(\\lambda\\) =0).\n\n\nResampling methods\n\nMonte Carlo\n(Other names: repeated learning-testing, repeated holdout, random subsampling)\nMonte Carlo sampling randomly shuffles the dataset (randomizing rows) and select the first 75% to be a training set and the remaining 25% as a validation set. This is sampling without replacement.\nThe validation error is averaged over n repetitions.\n\n\nBootstrap\nSimilar to Monte Carlo sampling where data are randomly selected from the dataset but with replacement.\nThe validation error is averaged over n repetitions.\n\n\n\n\n\n\nSunnys note\n\n\n\ncommonly (or by default) equal sampling probability?\n\n\n\n\nk-fold CV (Cross-validation)\nA dataset is divided into k partition at random (most commonly k=5 or 10). One partition is held out (set aside) as a validation set, and the remaining k-1 partitions of data are used as a training set. __The validation error_ is averaged over k repetitions.\nMost extreme case of k-fold CV, known as LOOCV (Leave One Out CV) is k = n where 1 observation is held out at each iteration.\n\n\nrepeated k-fold CV\nk-fold CV is done over n repetition and obtain \\({n} x {k}\\) validation errors.\nThe validation error is averaged over \\({n} \\times {k}\\)\n\n\n\n\n\n\nNote\n\n\n\nmost commonly use k=fold CV only once (rep =1) and repeated method is suggested as a way of obtaining more accurate and reliable estimates of error rates"
  },
  {
    "objectID": "posts/blog-post/erddap_private_dataset.html",
    "href": "posts/blog-post/erddap_private_dataset.html",
    "title": "Accessing ERDDAP private datasets programatically",
    "section": "",
    "text": "This guideline shows how to automate access to a private dataset on ERDDAP that requires Google login using Chrome.\n\nPrerequisites\n\nInstall Required Python Libraries\n\nRun the following command to install the necessary libraries:\n{bash}\n\npip install requests selenium webdriver_manager\n\n\nSetup WebDriver and Helper Functions\n\nSelenium is a tool that allows automated control of web browsers, such as Chrome, Firefox, and Edge. Here, we use it to navigate to the ERDDAP login page, handle Google login , and retrieve the authentication cookies.\nwebdriver_manager helps to automatically download and set up the appropriate WebDriver for Chrome.\n\n\nHelper Functions\nTo access the ERDDAP dataset, we define three helper functions:\nget_browser_cookies(login_url): Opens Chrome to log into the ERDDAP server and retrieves the login cookies. authenticate_session(login_url): Uses the cookies from get_browser_cookies to create an authenticated session. download_data(session, data_url, outfile): Uses the authenticated session to access the dataset URL and save it to the specified output file.\n\n\nCode\nHere is the code with comments for each part:\n{python}\n\nimport requests  # For managing sessions and HTTP requests\nfrom selenium import webdriver  # For browser control\nfrom webdriver_manager.chrome import ChromeDriverManager  # To manage the browser driver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\n\ndef get_browser_cookies(url):\n    \"\"\"Retrieve cookies from the browser using Selenium.\"\"\"\n    \n    # Set Chrome options to suppress automation messages\n    chrome_options = Options()\n    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Disable automation message\n    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  # Hide \"Chrome is controlled\" message\n    chrome_options.add_experimental_option(\"useAutomationExtension\", False)  # Disable the default automation extension\n\n    # Start the Chrome browser using webdriver-manager\n    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n\n    try:\n        # Navigate to the login page\n        driver.get(url)\n        \n        # Wait for the user to complete login (adjust time as needed)\n        time.sleep(60)  \n\n        # Retrieve cookies from the browser session\n        cookies = driver.get_cookies()\n        \n        # Verify cookies were retrieved successfully\n        if not cookies:\n            raise ValueError(\n                \"No cookies retrieved. Possible causes:\\n\"\n                \"- Insufficient sleep time; try increasing the sleep duration.\\n\"\n                \"- Incompatible or missing WebDriver for Chrome.\"\n            )\n\n        # Format cookies for session headers\n        formatted_cookies = \"; \".join([f\"{cookie['name']}={cookie['value']}\" for cookie in cookies])\n        print(\"Cookies Retrieved. Attempting to Access ERDDAP Dataset..\")\n        \n    finally:\n        # Close the browser\n        driver.quit()\n    \n    return formatted_cookies\n\ndef download_data(session, file_url, output_filename):\n    \"\"\"Download data file using the authenticated session.\"\"\"\n    response = session.get(file_url)\n\n    if response.status_code == 200:\n        print(\"Successfully downloaded data.\")\n\n        # Write the file content to disk\n        with open(output_filename, 'wb') as f:\n            f.write(response.content)\n        print(f\"Data saved to {output_filename}\")\n    else:\n        print(f\"Failed to download data. Status code: {response.status_code}\")\n        print(\"Response:\", response.text)\n\ndef authenticate_session(url):\n    \"\"\"Authenticate session and return the session object.\"\"\"\n    session = requests.Session()\n\n    # Get cookies from ERDDAP login page using Selenium\n    try:\n        cookie_header = get_browser_cookies(url)\n    except ValueError as e:\n        print(e)\n        exit(1)\n\n    # Set headers with cookies for authenticated requests\n    session.headers.update({\n        'User-Agent': 'Mozilla/5.0',\n        'Cookie': cookie_header\n    })\n    return session\n\n# Main Execution\nif __name__ == \"__main__\":\n\n    # ERDDAP login URL (through Google login)\n    login_url = \"https://polarwatch.noaa.gov/erddap/loginGoogle.html\"\n    \n    # ERDDAP data URL (direct link to dataset in .nc format)\n    data_url = [YOUR_ERDDAP_DATASET_URL]\n    \n    # Step 1: Authenticate Session\n    session = authenticate_session(login_url)\n\n    # Step 2: Download the data file and save as YOUR FILENAME\n    download_data(session, data_url, [FILENAME])\n\n\n\n\nStep-by-Step Usage\n\nInstall Required Libraries: Make sure requests, selenium, and webdriver_manager are installed.\nSet Login URL and Data URL:\n\nThe login_url is the ERDDAP login page (usually loginGoogle.html for Google login).\nThe data_url points to the specific dataset you want to download.\n\nRun the Script:\n\nRun the script to open Chrome, log in to ERDDAP, retrieve cookies, and download the specified dataset.\nAdjust time.sleep(60) in get_browser_cookies to allow enough time for login if needed.\n\n\n\n\nUsage Notes\n\nBrowser and Driver Compatibility: Ensure that ChromeDriver is compatible with your installed Chrome version. webdriver_manager handles this automatically, but Chrome must be installed.\nAlternative Browsers: This guide uses Chrome for simplicity, but Firefox and Edge are also supported with minor adjustments to the code.\nError Handling: The script checks if cookies were successfully retrieved. If not, it suggests potential issues."
  },
  {
    "objectID": "posts/blog-post/erddap_features.html",
    "href": "posts/blog-post/erddap_features.html",
    "title": "Useful ERDDAP features",
    "section": "",
    "text": "Features\n\nOutofDateDatasets provides outOfDateDatasets.html\n\nRun the following command to install the necessary libraries:\n{bash}\n\npip install requests selenium webdriver_manager\n\n\nSetup WebDriver and Helper Functions\n\nSelenium is a tool that allows automated control of web browsers (Chrome, Firefox, Edge, etc). Here, we use Seleminum to launch the ERDDAP login page, handle Google login, and retrieve the session cookies.\nwebdriver_manager helps to automatically download and set up the appropriate WebDriver (in this case Chrome).\n\n\nHelper Functions\nTo access the ERDDAP dataset, we define three helper functions:\n\nget_browser_cookies(login_url): Opens Chrome to log into the ERDDAP server and retrieves the session cookies.\nauthenticate_session(login_url): Uses the cookies from get_browser_cookies to create an authenticated session.\ndownload_data(session, data_url, outfile): Uses the authenticated session to access the dataset URL and save it to the specified output file.\n\n\n\nCode\nHere is the code with comments for each part:\n{python}\n\nimport requests  # For managing sessions and HTTP requests\nfrom selenium import webdriver  # For browser control\nfrom webdriver_manager.chrome import ChromeDriverManager  # To manage the browser driver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\n\ndef get_browser_cookies(url):\n    \"\"\"Retrieve cookies from the browser using Selenium.\"\"\"\n    \n    # Set Chrome options to suppress automation messages\n    chrome_options = Options()\n    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Disable automation message\n    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  # Hide \"Chrome is controlled\" message\n    chrome_options.add_experimental_option(\"useAutomationExtension\", False)  # Disable the default automation extension\n\n    # Start the Chrome browser using webdriver-manager\n    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n\n    try:\n        # Navigate to the login page\n        driver.get(url)\n        \n        # Wait for the user to complete login (adjust time as needed)\n        time.sleep(60)  \n\n        # Retrieve cookies from the browser session\n        cookies = driver.get_cookies()\n        \n        # Verify cookies were retrieved successfully\n        if not cookies:\n            raise ValueError(\n                \"No cookies retrieved. Possible causes:\\n\"\n                \"- Insufficient sleep time; try increasing the sleep duration.\\n\"\n                \"- Incompatible or missing WebDriver for Chrome.\"\n            )\n\n        # Format cookies for session headers\n        formatted_cookies = \"; \".join([f\"{cookie['name']}={cookie['value']}\" for cookie in cookies])\n        print(\"Cookies Retrieved. Attempting to Access ERDDAP Dataset..\")\n        \n    finally:\n        # Close the browser\n        driver.quit()\n    \n    return formatted_cookies\n\ndef download_data(session, file_url, output_filename):\n    \"\"\"Download data file using the authenticated session.\"\"\"\n    response = session.get(file_url)\n\n    if response.status_code == 200:\n        print(\"Successfully downloaded data.\")\n\n        # Write the file content to disk\n        with open(output_filename, 'wb') as f:\n            f.write(response.content)\n        print(f\"Data saved to {output_filename}\")\n    else:\n        print(f\"Failed to download data. Status code: {response.status_code}\")\n        print(\"Response:\", response.text)\n\ndef authenticate_session(url):\n    \"\"\"Authenticate session and return the session object.\"\"\"\n    session = requests.Session()\n\n    # Get cookies from ERDDAP login page using Selenium\n    try:\n        cookie_header = get_browser_cookies(url)\n    except ValueError as e:\n        print(e)\n        exit(1)\n\n    # Set headers with cookies for authenticated requests\n    session.headers.update({\n        'User-Agent': 'Mozilla/5.0',\n        'Cookie': cookie_header\n    })\n    return session\n\n# Main Execution\nif __name__ == \"__main__\":\n\n    # ERDDAP login URL (through Google login)\n    login_url = \"https://polarwatch.noaa.gov/erddap/loginGoogle.html\"\n    \n    # ERDDAP data URL (direct link to dataset in .nc format)\n    data_url = [YOUR_ERDDAP_DATASET_URL]\n    \n    # Step 1: Authenticate Session\n    session = authenticate_session(login_url)\n\n    # Step 2: Download the data file and save as YOUR FILENAME\n    download_data(session, data_url, [FILENAME])\n\n\n\n\nStep-by-Step Usage\n\nInstall Required Libraries: Make sure requests, selenium, and webdriver_manager are installed.\nSet Login URL and Data URL:\n\nThe login_url is the ERDDAP login page (usually loginGoogle.html for Google login).\nThe data_url points to the specific dataset you want to download.\n\nRun the Script:\n\nRun the script to open Chrome, log in to ERDDAP, retrieve cookies, and download the specified dataset.\nAdjust time.sleep(60) in get_browser_cookies to allow enough time for login if needed.\n\n\n\n\nUsage Notes\n\nBrowser and Driver Compatibility: Ensure that ChromeDriver is compatible with your installed Chrome version. webdriver_manager handles this automatically, but Chrome must be installed.\nAlternative Browsers: This guide uses Chrome for simplicity, but Firefox and Edge are also supported with minor adjustments to the code.\nError Handling: The script checks if cookies were successfully retrieved. If not, it suggests potential issues."
  },
  {
    "objectID": "posts/blog-post/jsdocs.html",
    "href": "posts/blog-post/jsdocs.html",
    "title": "JS Docs",
    "section": "",
    "text": "I find javascript files difficult to read and follow especially when they were written by someone else and your job is to modify or fix what isn’t working. Often the script contains thousands of lines and it’s unclear which objects the javascript actions are applied to.\nThere are some good comments in the scripts but not in a consistent format recommended by standards. In this situation, I find it easy to first generate web-based documents using JSDoc and start from there.\nSince I don’t the admin access, I downloaded and installed locally\nnpm install jsdoc\njsdocs myjavascript.js\nThis created a webpage reference to all functions with any comments or annotations."
  },
  {
    "objectID": "posts/blog-post/conf_resources.html",
    "href": "posts/blog-post/conf_resources.html",
    "title": "Past Conference Recordings",
    "section": "",
    "text": "SciPy Conference, Tacoma WA YouTube\nPosit:Conf"
  },
  {
    "objectID": "posts/blog-post/conf_resources.html#conferences-and-workshops-for-data-science",
    "href": "posts/blog-post/conf_resources.html#conferences-and-workshops-for-data-science",
    "title": "Past Conference Recordings",
    "section": "",
    "text": "SciPy Conference, Tacoma WA YouTube\nPosit:Conf"
  },
  {
    "objectID": "posts/blog-post/projection.html",
    "href": "posts/blog-post/projection.html",
    "title": "Coordinate System",
    "section": "",
    "text": "A Coordinate Reference System (CRS) is a framework used to represent the locations of geographic features, imagery, or observations. It enables spatial data to be accurately positioned and interpreted in relation to the Earth.\nEach CRS is defined by the following elements:\n\nMeasurement Framework\n\nGeographic: Coordinates measured on a spherical or ellipsoidal surface (latitude and longitude, referenced to the Earth’s center).\nProjected: Coordinates projected onto a two-dimensional (planar) surface, typically using a mathematical transformation (map projection).\n\nUnit of Measurement\n\nGeographic CRS: Uses decimal degrees for latitude and longitude.\nProjected CRS: Uses linear units such as meters or feet.\n\nMap Projection Definition (for Projected CRS)\n\nSpecifies how the three-dimensional surface of the Earth is translated onto a two-dimensional plane. This includes the mathematical projection method and its parameters.\n\nOther System Properties\n\nIncludes the reference ellipsoid (spheroid), geodetic datum, and projection parameters such as standard parallels, central meridian, and false easting/northing (x and y shifts)."
  },
  {
    "objectID": "posts/blog-post/projection.html#what-is-coordinate-system",
    "href": "posts/blog-post/projection.html#what-is-coordinate-system",
    "title": "Coordinate System",
    "section": "",
    "text": "A Coordinate Reference System (CRS) is a framework used to represent the locations of geographic features, imagery, or observations. It enables spatial data to be accurately positioned and interpreted in relation to the Earth.\nEach CRS is defined by the following elements:\n\nMeasurement Framework\n\nGeographic: Coordinates measured on a spherical or ellipsoidal surface (latitude and longitude, referenced to the Earth’s center).\nProjected: Coordinates projected onto a two-dimensional (planar) surface, typically using a mathematical transformation (map projection).\n\nUnit of Measurement\n\nGeographic CRS: Uses decimal degrees for latitude and longitude.\nProjected CRS: Uses linear units such as meters or feet.\n\nMap Projection Definition (for Projected CRS)\n\nSpecifies how the three-dimensional surface of the Earth is translated onto a two-dimensional plane. This includes the mathematical projection method and its parameters.\n\nOther System Properties\n\nIncludes the reference ellipsoid (spheroid), geodetic datum, and projection parameters such as standard parallels, central meridian, and false easting/northing (x and y shifts)."
  },
  {
    "objectID": "posts/blog-post/projection.html#types-of-coordinate-system",
    "href": "posts/blog-post/projection.html#types-of-coordinate-system",
    "title": "Coordinate System",
    "section": "Types of Coordinate System",
    "text": "Types of Coordinate System\nThere are two common types:\nGeographic Coordinate Reference System (Geographic CRS)\nA Geographic CRS uses latitude and longitude to specify locations on the Earth’s surface, referencing positions on a three-dimensional spherical or ellipsoidal model of the planet. It is defined by:\n\nAn angular unit of measurement: Typically degrees for both latitude and longitude.\nA prime meridian: The zero-longitude line from which other longitudes are measured (commonly the Greenwich Meridian).\nA geodetic datum: A mathematical model of the Earth’s shape (based on a spheroid or ellipsoid) that provides a frame of reference for the coordinates.\n\nProjected Coordinate Reference System (Projected CRS)\nA Projected CRS transforms the Earth’s curved surface (as defined by a geographic CRS) onto a flat, two-dimensional Cartesian plane using a map projection. Every projected CRS is based on an underlying geographic CRS (including its datum and spheroid). Locations in a projected CRS are identified by:\n\nX and Y coordinates: These represent positions on a grid, measured in linear units (such as meters or feet).\nGrid origin: The origin (0,0) of the coordinate system, which may or may not be at the center of the projection area, depending on the specific projection parameters chosen.\n\n\n\n\n\n\n\nCartesian Coordinate Plane\n\n\n\nA Cartesian plane assumes:\n\nRight-angle (orthogonal) x and y axes\nUniform scale throughout the plane\nLocations are measured relative to an origin (0,0)"
  },
  {
    "objectID": "posts/blog-post/projection.html#shape-of-the-earth-geodesy",
    "href": "posts/blog-post/projection.html#shape-of-the-earth-geodesy",
    "title": "Coordinate System",
    "section": "Shape of the Earth (geodesy)",
    "text": "Shape of the Earth (geodesy)\nConcept of Spheroid and Datum\n\nSpheroid is a shape that approximates the shape of the earth.\nDatum is a mathematical model that defines the position of the spheroid relative to the center of the Earth. Datum describes how coordiate system is alligned with the Earth.\nThere are two main types of Datum\n\nGeodetic (or geographic) datum : defines origin and orientation of latitude and longitude.\nVertical datum : defines elevation or depth (sea level)\n\nDatum includes 2 things: A Spheroid and Reference Point\nLocations CRS changes depending on which datum and spheroid are used to describe locations."
  },
  {
    "objectID": "posts/blog-post/projection.html#three-main-datums",
    "href": "posts/blog-post/projection.html#three-main-datums",
    "title": "Coordinate System",
    "section": "Three Main Datums",
    "text": "Three Main Datums\nBasically each datum has slightly different shape of the earth ellipsoid and the reference point.\n\n\n\n\n\n\n\n\n\n\n\nDatum\nSpheroid\nSemi-major axis (m)\nFlattening\nReference Point\nNotes\n\n\n\n\nWGS84\nWGS84 Ellipsoid\n6,378,137.0\n1/298.257223563\nEarth’s center of mass\nStandard for GPS\n\n\nNAD83\nGRS80 Ellipsoid\n6,378,137.0\n1/298.257222101\nEarth’s center of mass\nUsed for US/Canada; similar to WGS84, not identical.\n\n\nNAD27\nClarke 1866 Ellipsoid\n6,378,206.4\n1/294.9786982\nMeades Ranch, Kansas, USA\nLocal reference; older maps.\n\n\n\nNext blog will describe and compare few main polar projections."
  },
  {
    "objectID": "posts/blog-post/cf-convention.html",
    "href": "posts/blog-post/cf-convention.html",
    "title": "CF Convention forTime",
    "section": "",
    "text": "must include units (see format)\nhas no default value\ntime coordinate and the datetime are interconvertible given the calendar\nmust comprise units of measure, “since”, and a reference datetime. (“days sinced 1990-01-01”)\n\nacceptable unit of measures (day (h), hour (hr, h), minute (min), and second (sec, s). Plurals are acceptable\n\nleap seconds should be properly dealt but often not the case. If using standard, proleptic_gregorian, and julian for calenar, one can add units_metadata to indicate “leap_seconds”\n\n\n\ndouble time(time) ;\n  time:axis = \"T\"; // optional\n  time:standard_name = \"time\" ; // optional\n  time:units = \"days since 1990-1-1 0:0:0\" ; // mandatory\nExample of how adding leap second information\n  float time_stdnone ;\n    time_stdnone:standard_name = \"time\" ;\n    time_stdnone:long_name = \"Model data with no leap seconds\" ;\n    time_stdnone:calendar = \"standard\" ;\n    time_stdnone:units = \"seconds since 2016-12-31 23:59:58\" ;\n    time_stdnone:units_metadata = \"leap_seconds: none\" ;\n  float time_stdutc ;\n    time_stdutc:standard_name = \"time\" ;\n    time_stdutc:long_name = \"Model data with leap seconds or obs data with accurate UTC\" ;\n    time_stdutc:calendar = \"standard\" ;\n    time_stdutc:units = \"seconds since 2016-12-31 23:59:58\" ;\n    time_stdutc:units_metadata = \"leap_seconds: utc\" ;"
  },
  {
    "objectID": "posts/blog-post/cf-convention.html#time-variable",
    "href": "posts/blog-post/cf-convention.html#time-variable",
    "title": "CF Convention forTime",
    "section": "",
    "text": "must include units (see format)\nhas no default value\ntime coordinate and the datetime are interconvertible given the calendar\nmust comprise units of measure, “since”, and a reference datetime. (“days sinced 1990-01-01”)\n\nacceptable unit of measures (day (h), hour (hr, h), minute (min), and second (sec, s). Plurals are acceptable\n\nleap seconds should be properly dealt but often not the case. If using standard, proleptic_gregorian, and julian for calenar, one can add units_metadata to indicate “leap_seconds”\n\n\n\ndouble time(time) ;\n  time:axis = \"T\"; // optional\n  time:standard_name = \"time\" ; // optional\n  time:units = \"days since 1990-1-1 0:0:0\" ; // mandatory\nExample of how adding leap second information\n  float time_stdnone ;\n    time_stdnone:standard_name = \"time\" ;\n    time_stdnone:long_name = \"Model data with no leap seconds\" ;\n    time_stdnone:calendar = \"standard\" ;\n    time_stdnone:units = \"seconds since 2016-12-31 23:59:58\" ;\n    time_stdnone:units_metadata = \"leap_seconds: none\" ;\n  float time_stdutc ;\n    time_stdutc:standard_name = \"time\" ;\n    time_stdutc:long_name = \"Model data with leap seconds or obs data with accurate UTC\" ;\n    time_stdutc:calendar = \"standard\" ;\n    time_stdutc:units = \"seconds since 2016-12-31 23:59:58\" ;\n    time_stdutc:units_metadata = \"leap_seconds: utc\" ;"
  },
  {
    "objectID": "posts/blog-post/cf-convention.html#time-calendar",
    "href": "posts/blog-post/cf-convention.html#time-calendar",
    "title": "CF Convention forTime",
    "section": "Time Calendar",
    "text": "Time Calendar\n\nCF calendar refers to datetime, not just date.\ncalendar standard: default UDUNITS. A deprecated alternative name is gregorian.\nother calendar categories are proleptic_gregorian, julian, and utc. Read more at cf-convention"
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html",
    "href": "posts/blog-post/posit-conf24.html",
    "title": "Posit:Conf 2024",
    "section": "",
    "text": "This was my first Posit Conference, and I was eager to learn about new technologies and tools. Interestingly, what I enjoyed most were the keynote speakers and the spontaneous conversations during breaks.\nThe keynote speakers were inspirational, offering a big-picture perspective and motivations for pursuing a meaningful career. The impromptu discussions with conference attendees—who came from diverse backgrounds gave me valuable insights, not just in data science, but in life.\nHaving had this wonderful opportunity to attend, I’d like to share a few key takeways. This blog isn’t about the specific topics covered in the conference session, but rather about what I took away from the overall experience."
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#keynote-take-away",
    "href": "posts/blog-post/posit-conf24.html#keynote-take-away",
    "title": "Posit:Conf 2024",
    "section": "Keynote take-away",
    "text": "Keynote take-away\nAt times work can settle into comfortable routines or become a pursuit of personal intellectual satisfaction.\nWhile goals and objectives guide our work and shape our workflows, I often reflect on what the work truly means to me.\nSome of the keynote messages resonated with me as questions i can ask myself.\n\nHow can we be innovative to make things better?\nWhatever I do, aim to contribute–even in small ways–to creating positive change.\nCreate Virtuous cycles to benefit everyone involved. Think of ways to reinvest my benefits back into the community\nOpen Science: Use data to understand the world better. Then we know how to make it better\nNegative bias is a serious threat to our well-being and ability to address the problems we face\nWorking with people from different background can be challenging.\nAgree on a set of core values such as radical transparency, trust, respect, courage, commitment"
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#sessions-conversation-take-away",
    "href": "posts/blog-post/posit-conf24.html#sessions-conversation-take-away",
    "title": "Posit:Conf 2024",
    "section": "Sessions & Conversation take-away",
    "text": "Sessions & Conversation take-away\n\nWorking with people from different backgrounds can be challenging. It’s important to agree on a set of core values, such as radical transparency, trust, respect, courage, and commitment.\nChatGPT is becoming widely used, especially among the new generation of professionals. Traditional software engineering interviews often involve requesting code samples, but this practice is becoming less relevant as many candidates now use ChatGPT. What does the future hold? Perhaps it lies with those who can construct the “right” questions to solve difficult problems."
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#key-statiscal-and-machine-learning-tools",
    "href": "posts/blog-post/posit-conf24.html#key-statiscal-and-machine-learning-tools",
    "title": "Posit:Conf 2024",
    "section": "Key Statiscal and Machine Learning tools",
    "text": "Key Statiscal and Machine Learning tools\n\n{marginaleffects} package for model comparisons. you write a comparison function and the package help to generate ggplot and other output to compare models.\nPrediction Intervals: github.com/brshallo/posit-2024, talked about different ways to construct and evaluate prediction intervals\n\nInterval width is widely used and good to evaluate if a model is improving as the width becomes narrow. (Constant variance desired)\nin Conformed prediction, the assumption is relaxed, and you compute upper and lower bands. (manokhin molar). Use train, calibration, and test datasets for estimating intervals.\n\nDeep learning: {keras3} allows the use of keras and tensorflow in python throough reticulate.\n\ncreate neural network architecture just like python keras.\noffers the interface to GPU.\nsave the models in *.keras to change the backend for different cycles. (save and read model)\ndistributed training\nkeras3.posit.co\n\n\n*Avoid garbage in/garbage out: Use interactive tool like shiny for data cleaning"
  },
  {
    "objectID": "posts/blog-post/posit-conf24.html#tools-to-check-out",
    "href": "posts/blog-post/posit-conf24.html#tools-to-check-out",
    "title": "Posit:Conf 2024",
    "section": "Tools to check out",
    "text": "Tools to check out\n\nWeb/Report publishing : Quarto Dashboard for web dashboard and quarto PDF using typst (faster than latex)\nTypst : Does Design Matter? Studies show aesthetically pleasant design make things more useful and builds trust. Make reports beautiful, reproducible, parameterized, automatically generated\nEducation/Training : WebR/Quarto live for interactive web interface with programming language, ideal for training. It can be either a standalone web application or a code chunk in a quarto document.\nCloud Computing: Posit Workbench is a cloud computing and it handles infrastrcuture manageement and security requirements. It can be deployed in Posit:Connect.\nCloud and Collaboration: Github codespace for computing and sharing simultaneously and is free for # of hours and you can use docker image to set a virtual environment.\n{pins}: customizing and automating report generation and delivery"
  },
  {
    "objectID": "posts/blog-post/nco_cheatsheet.html",
    "href": "posts/blog-post/nco_cheatsheet.html",
    "title": "{nco} cheatsheet",
    "section": "",
    "text": "This is an instruction to create a netcdf file with templated metadata.\n\nCreate a metadata template from an existing nc file\nncdump samplefile.nc &gt; metadata_header.txt\nncks -A -v temperature newdata.nc newfile.nc\nncdump -v temperature newfile.nc\nncks -A newdata.nc newfile.nc"
  },
  {
    "objectID": "posts/blog-post/pycascade24.html",
    "href": "posts/blog-post/pycascade24.html",
    "title": "PyCascades 2024",
    "section": "",
    "text": "Typing concepts/categories\n\nstrongly vs weakly (“loosely”)\ndynamically vs statically\nDuck typing and use of dunder\n\nPython best practices\n\nPython likes protocols/strongly type (type hinting/duck typing)\nAnnotate it as concrete as possible\nDeclarative data Types\n\n    def func() -&gt; list:  \n    def func() -&gt; list[str]:\n\nPermissive input types and strict return types\nUse standard libraries\nIf you are creating for publishing for other people, more typed the better\nBut.. if it doesn’t work for you, don’t do it.\n\n\n\n\n\nFrom bugs, results and analysis of the bugs, we can learn so much and improve coding practices. The speaker presented 3 major bugs with corresponding results and reasons.\n\nESA Ariane5 explosion: Loss of over 300mil dollars and research time\n\nWhy: function reused from 4th project in the 5th project with assumption and without examination. Backup system failed to work.\nLesson : we use software package for multiple projects without examining requirements.\n\nAT&T Long distance: simple update right before release triggered malfunction remedy and kept being restarted\n\nWhy: “self healing” feature added without understanding the entire system and ramification\nLesson: (sunnys note) documentation is important for collaborators\n\nPatrio sysstem failure: Ground to air missle to intercept in-coming missle failed and 28 killed and hundreds injured\n\nWhy: calculation of location failed due to not applying subroutine. Some error occurred but could not be replicated thus ignored.\nLesson: when encounter errors, don’t just ignore.\nfind out what causes it. Test any edge case. Human intervention and supervision when needed.\n\n\n\n\n\nTalk was about the concept of introspection in python and functions to examine.\n(sunny) It may be a good practice to write a set of important methods for introspection.\n\nhelp() It is used it to find what other functions do\nhasattr() Checks if an object has an attribute\ngetattr() Returns the contents of an attribute if there are some.\nrepr() Return string representation of object\ncallable() Checks if an object is a callable object (a function)or not.\nissubclass() Checks if a specific class is a derived class of another class.\nisinstance() Checks if an objects is an instance of a specific class.\nsys() Give access to system specific variables and functions\n__doc__ Return some documentation about an object\n__name__ Return the name of the object.\n\n\n\n\n\njupyternotebook as serverless\nexample shows a web service deployed on fastAPI framework on Azure\n\n\n\n\n\nMLOps life cycle includes data prep, model train, evaluation, deployment, monitoring, and maintenance.\nWhy use MLOps?\n\nto speed deployment process\nto enhance collaboration\nto make it scalable\nto support governance\nfor continuous training\n\nChallenges faced with just model versioning to track and manage over time\n\ncomponents: artifact, version\nstorage overhead\ncomplex dependencies\nData issues: when data drift occurs, there’s a change in distribution of model based on input data, degrading performance\n\nDetection methods - Stat test/Performance monitoring/Visualization\n\nPerformance monitoring\n\nCatch issues early\nCreate metrics with accuracy, latency/throughput, business metrics, user engagement, etc.\n\nSet alert notificiation\nCreate a dashboard\n\nContinuous Training\n\nRegular updatres of model to adapt to new data patterns\nAuto retraining/incremental learning\n\nCollaboration - in all levels (data science, data/software engineering, devOps)\nBest practices\n\nStart small to scale\nEmbrace experiment\nFocus on quality data\nSecurity and compliance\nContinuous monitoring/retrain\nRight tools\n\n\n\n\n\nPresenter demonstrated creation of pygame and how easy it can be to create a quick and simple graphic with a moving object using pygame package.\n\n\n\n\n\n\nTODO\n\n\n\nMore talks to cover\n\nproblem solving by dissolution (Grothendieck and Jean-Pierre Serre, author of A Course in Arithmatic in pdf)\nmore on dunder\ntesting with playwright\npython & rust\npyscript\nusing K-means to play with the colors of photos\nlightning talks (nasa earth data, quarto from posit developer)\nGraphQL Operations\nReproducibility with and without docker\nAPI + cli\nNotably inaccessible (Jupyter)\ncircuit python (microcontroller)\ncontainerizing python\nCircuitPython for microcontrollers\nsprint (py opensci)\n\nOther topics\n\nhttps://temporal.io. perhaps solution for error handling when requesting data through erddap? open source/can be used with php, python, java, etc.\ncasual talk with fellow attendee on github rebase and grouping commits and random forest algorithm"
  },
  {
    "objectID": "posts/blog-post/pycascade24.html#pycascade-2024",
    "href": "posts/blog-post/pycascade24.html#pycascade-2024",
    "title": "PyCascades 2024",
    "section": "",
    "text": "Typing concepts/categories\n\nstrongly vs weakly (“loosely”)\ndynamically vs statically\nDuck typing and use of dunder\n\nPython best practices\n\nPython likes protocols/strongly type (type hinting/duck typing)\nAnnotate it as concrete as possible\nDeclarative data Types\n\n    def func() -&gt; list:  \n    def func() -&gt; list[str]:\n\nPermissive input types and strict return types\nUse standard libraries\nIf you are creating for publishing for other people, more typed the better\nBut.. if it doesn’t work for you, don’t do it.\n\n\n\n\n\nFrom bugs, results and analysis of the bugs, we can learn so much and improve coding practices. The speaker presented 3 major bugs with corresponding results and reasons.\n\nESA Ariane5 explosion: Loss of over 300mil dollars and research time\n\nWhy: function reused from 4th project in the 5th project with assumption and without examination. Backup system failed to work.\nLesson : we use software package for multiple projects without examining requirements.\n\nAT&T Long distance: simple update right before release triggered malfunction remedy and kept being restarted\n\nWhy: “self healing” feature added without understanding the entire system and ramification\nLesson: (sunnys note) documentation is important for collaborators\n\nPatrio sysstem failure: Ground to air missle to intercept in-coming missle failed and 28 killed and hundreds injured\n\nWhy: calculation of location failed due to not applying subroutine. Some error occurred but could not be replicated thus ignored.\nLesson: when encounter errors, don’t just ignore.\nfind out what causes it. Test any edge case. Human intervention and supervision when needed.\n\n\n\n\n\nTalk was about the concept of introspection in python and functions to examine.\n(sunny) It may be a good practice to write a set of important methods for introspection.\n\nhelp() It is used it to find what other functions do\nhasattr() Checks if an object has an attribute\ngetattr() Returns the contents of an attribute if there are some.\nrepr() Return string representation of object\ncallable() Checks if an object is a callable object (a function)or not.\nissubclass() Checks if a specific class is a derived class of another class.\nisinstance() Checks if an objects is an instance of a specific class.\nsys() Give access to system specific variables and functions\n__doc__ Return some documentation about an object\n__name__ Return the name of the object.\n\n\n\n\n\njupyternotebook as serverless\nexample shows a web service deployed on fastAPI framework on Azure\n\n\n\n\n\nMLOps life cycle includes data prep, model train, evaluation, deployment, monitoring, and maintenance.\nWhy use MLOps?\n\nto speed deployment process\nto enhance collaboration\nto make it scalable\nto support governance\nfor continuous training\n\nChallenges faced with just model versioning to track and manage over time\n\ncomponents: artifact, version\nstorage overhead\ncomplex dependencies\nData issues: when data drift occurs, there’s a change in distribution of model based on input data, degrading performance\n\nDetection methods - Stat test/Performance monitoring/Visualization\n\nPerformance monitoring\n\nCatch issues early\nCreate metrics with accuracy, latency/throughput, business metrics, user engagement, etc.\n\nSet alert notificiation\nCreate a dashboard\n\nContinuous Training\n\nRegular updatres of model to adapt to new data patterns\nAuto retraining/incremental learning\n\nCollaboration - in all levels (data science, data/software engineering, devOps)\nBest practices\n\nStart small to scale\nEmbrace experiment\nFocus on quality data\nSecurity and compliance\nContinuous monitoring/retrain\nRight tools\n\n\n\n\n\nPresenter demonstrated creation of pygame and how easy it can be to create a quick and simple graphic with a moving object using pygame package.\n\n\n\n\n\n\nTODO\n\n\n\nMore talks to cover\n\nproblem solving by dissolution (Grothendieck and Jean-Pierre Serre, author of A Course in Arithmatic in pdf)\nmore on dunder\ntesting with playwright\npython & rust\npyscript\nusing K-means to play with the colors of photos\nlightning talks (nasa earth data, quarto from posit developer)\nGraphQL Operations\nReproducibility with and without docker\nAPI + cli\nNotably inaccessible (Jupyter)\ncircuit python (microcontroller)\ncontainerizing python\nCircuitPython for microcontrollers\nsprint (py opensci)\n\nOther topics\n\nhttps://temporal.io. perhaps solution for error handling when requesting data through erddap? open source/can be used with php, python, java, etc.\ncasual talk with fellow attendee on github rebase and grouping commits and random forest algorithm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "How to embed video in github page\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJul 9, 2025\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nCoordinate System\n\n\n\nGIS\n\n\n\nNotes on coordinate system (CRS), types of CRS, Geodesy, Spheroid, Datums\n\n\n\n\n\nJun 26, 2025\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nCF Convention forTime\n\n\n\nmetadata\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nUseful ERDDAP features\n\n\n\nerddap\n\ndata management\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nAccessing ERDDAP private datasets programatically\n\n\n\npython\n\n\n\nInstruction to Access a Private Dataset on ERDDAP Programmatically Using Python with Selenium\n\n\n\n\n\nNov 7, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPast Conference Recordings\n\n\n\nworkshops\n\nopenscience\n\npython\n\nR\n\n\n\nFor those who are interested in past conference recordings\n\n\n\n\n\nSep 27, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPast Conference Recordings\n\n\n\nworkshops\n\nopenscience\n\npython\n\nR\n\n\n\nFor those who are interested in past conference recordings\n\n\n\n\n\nSep 27, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nJS Docs\n\n\n\ndocumentation\n\njavascript\n\n\n\nHow I use it to organize and view existing javascript files\n\n\n\n\n\nSep 6, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPosit:Conf 2024\n\n\n\nworkshops\n\nopenscience\n\npython\n\nR\n\n\n\nWhat I’ve learned from Posit Conference\n\n\n\n\n\nAug 14, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPyCascades 2024\n\n\n\nworkshops\n\nopenscience\n\npython\n\n\n\nPersonal notes on the presentations and lessons learned\n\n\n\n\n\nApr 10, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nSet up JupyterHub on AWS\n\n\n\nopenscience\n\ncloud\n\n\n\nHow to set up JupyterHub on AWS (Amazon web services)\n\n\n\n\n\nApr 3, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\n{nco} cheatsheet\n\n\n\nnetcdf\n\nnco\n\n\n\nWorking with and nco tool\n\n\n\n\n\nApr 2, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up STAC on AWS\n\n\n\nnetcdf\n\nnco\n\n\n\nSetting up STAC using stac-fastapi\n\n\n\n\n\nApr 2, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nLit Review: Comparing resampling methods\n\n\n\nmachine learning\n\nliterature review\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nSunny Hospital\n\n\n\n\n\n\n\n\n\n\n\n\nPython Iterators, Iterables, Asynchronous\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 22, 2021\n\n\nSunny Hospital\n\n\n\n\n\nNo matching items"
  }
]